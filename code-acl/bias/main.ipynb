{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"cells":[{"cell_type":"code","metadata":{"id":"S4LZFyZ6bB9d","executionInfo":{"status":"ok","timestamp":1637684647753,"user_tz":-480,"elapsed":3002,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}}},"source":["import tensorflow as tf\n","from tensorflow import keras"],"id":"S4LZFyZ6bB9d","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-8-eBsqCuph","executionInfo":{"status":"ok","timestamp":1637684647754,"user_tz":-480,"elapsed":19,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"2f636230-f50b-4404-b721-ffa76e422597"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"id":"Q-8-eBsqCuph","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Nov 23 16:24:06 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZ-6yxnzrHqo","executionInfo":{"status":"ok","timestamp":1637684647754,"user_tz":-480,"elapsed":10,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"89225a4f-35f6-4258-d00e-b4668423d462"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"id":"WZ-6yxnzrHqo","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 54.8 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUBi_CsJeOQe","executionInfo":{"status":"ok","timestamp":1637684647755,"user_tz":-480,"elapsed":9,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"80f2819c-7733-40b7-d889-0f7527b76a31"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"QUBi_CsJeOQe","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"om0_6fADizPI","executionInfo":{"status":"ok","timestamp":1637684647755,"user_tz":-480,"elapsed":7,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}}},"source":["import sys\n","import os\n","os.chdir('/content/drive/MyDrive/CS105BProject/bias' )\n","sys.path.append('/content/drive/MyDrive/CS105BProject')\n","sys.path.append(os.getcwd())"],"id":"om0_6fADizPI","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS-Uo5DdnhH9","executionInfo":{"status":"ok","timestamp":1637684653206,"user_tz":-480,"elapsed":5458,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"6af59ce8-5628-45b6-8285-7bd48ce1b2ac"},"source":["!pip install transformers\n","!pip install pytorch-nlp\n","# !pip install hypopt"],"id":"JS-Uo5DdnhH9","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp) (4.62.3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eba4c237","executionInfo":{"status":"ok","timestamp":1637684657017,"user_tz":-480,"elapsed":3831,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"23d41d6b-145e-4801-f1bd-751c8c6470d0"},"source":["import sys\n","import os\n","# sys.path.append('../../code-acl')\n","# sys.path.append(os.getcwd())\n","sys.path.append('/content/drive/MyDrive/CS105BProject/bias/')\n","os.environ['OMP_NUM_THREADS'] = \"1\"\n","import argparse\n","import pandas as pd\n","import pickle\n","from model.generator import TransformerDataset, transformer_collate\n","from model.bertmodel import MyBertModel\n","from model.lstmmodel import LSTMModel\n","import torch\n","from parameters import BERT_MODEL_PATH, CLAIM_ONLY, CLAIM_AND_EVIDENCE, EVIDENCE_ONLY, DEVICE, INPUT_TYPE_ORDER\n","from transformers import AdamW\n","import numpy as np\n","from utils.utils import print_message, clean_str, preprocess\n","from sklearn.metrics import f1_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from collections import Counter\n","from torchnlp.word_to_vector import GloVe\n","from collections import Counter\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","#from hypopt import GridSearch\n","from model_selection import GridSearch\n","from tqdm import tqdm\n","\n","def load_data(dataset):\n","    #path = \"../../multi_fc_publicdata/\" + dataset + \"/\"\n","\n","    path = \"../multi_fc_publicdata/\" + dataset + \"/\"\n","\n","    main_data = pd.read_csv(path + dataset + \".tsv\", sep=\"\\t\", header=None)\n","    for index, row in main_data.iterrows():\n","      main_data[1][index] = preprocess(row[1])\n","    \n","    \n","    snippets_data = pd.read_csv(path + dataset + \"_snippets.tsv\", sep=\"\\t\", header=None)    \n","    for index, row in snippets_data.iterrows():\n","      snippets_data[1][index] = preprocess(row[1])\n","      snippets_data[2][index] = preprocess(row[2])\n","      snippets_data[3][index] = preprocess(row[3])\n","      snippets_data[4][index] = preprocess(row[4])\n","      snippets_data[5][index] = preprocess(row[5])\n","      snippets_data[6][index] = preprocess(row[6])\n","      snippets_data[7][index] = preprocess(row[7])\n","      snippets_data[8][index] = preprocess(row[8])\n","      snippets_data[9][index] = preprocess(row[9])\n","      snippets_data[10][index] = preprocess(row[10])\n","    \n","    label_order = pickle.load(open(path + dataset + \"_labels.pkl\", \"rb\"))\n","    splits = pickle.load(open(path + dataset + \"_index_split.pkl\", \"rb\"))\n","\n","    return main_data, snippets_data, label_order, splits\n","\n","def make_generators(main_data, snippets_data, label_order, splits, params, dataset_generator=TransformerDataset, other_dataset=False):\n","    generators = []\n","\n","    all_labels = main_data.values[:,2]\n","    counter = Counter(all_labels)\n","    ss = \"\"\n","    for c in label_order:\n","        ss = ss + \", \" + str(c) + \" (\" + str(np.around(counter[c]/len(all_labels) * 100,1)) + \"\\%)\"\n","        #print(c, np.around(counter[c]/len(all_labels) * 100,1), \"%\", counter[c])\n","    print(\"len\", len(all_labels), ss)\n","\n","    for isplit, split in enumerate(splits):\n","        # print(f'isplit {isplit}')\n","        sub_main_data = main_data.values[split]\n","        # print(f'len sub_main_data: {len(sub_main_data)}')\n","        \n","        sub_snippets_data = snippets_data.values[split]\n","        # print(f'len sub_snippets_data: {len(sub_snippets_data)}')\n","\n","        \n","\n","        tmp = dataset_generator(sub_main_data, sub_snippets_data, label_order)\n","        if isplit == 0:\n","            generator = torch.utils.data.DataLoader(tmp, **params[0])\n","        else:\n","            generator = torch.utils.data.DataLoader(tmp, **params[1])\n","\n","        generators.append(generator)\n","\n","        # print(sub_main_data)\n","        # print(sub_snippets_data)\n","        # print(f'tmp: \\n {tmp[0]}')\n","        # gen0 = next(iter(generator))\n","        # print(f'gen0: \\n {gen0}')\n","\n","\n","    # make class weights\n","    labels = main_data.values[splits[0]][:,2]\n","    labels = np.array([label_order.index(v) for v in labels])\n","\n","\n","    if not other_dataset:\n","        label_weights = torch.tensor(compute_class_weight(\"balanced\", classes=np.arange(len(label_order)), y=labels).astype(np.float32))\n","    else:\n","        label_weights = None\n","\n","    return generators[0], generators[1], generators[2], label_weights\n","\n","def evaluate(generator, model, other_from=None, ignore_snippet=None):\n","    all_labels = []\n","    all_predictions = []\n","\n","    all_claimIDs = []\n","    all_logits = []\n","\n","    for vals in generator:\n","        claimIDs, claims, labels, snippets = vals[0], vals[1], vals[2], vals[3]\n","\n","        if ignore_snippet is not None:\n","            for i in range(len(snippets)):\n","                snippets[i][ignore_snippet] = \"filler\"\n","\n","        all_labels += labels\n","        logits = model(claims, snippets)\n","\n","        predictions = torch.argmax(logits, 1).cpu().numpy()\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            predictions[predictions == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        all_predictions += predictions.tolist()\n","\n","        all_claimIDs += claimIDs\n","        all_logits += logits.cpu().numpy().tolist()\n","\n","    f1_micro = f1_score(all_labels, all_predictions, average=\"micro\")\n","    f1_macro = f1_score(all_labels, all_predictions, average=\"macro\")\n","\n","    return f1_micro, f1_macro, all_claimIDs, all_logits, all_labels, all_predictions\n","\n","def train_step(optimizer, vals, model, criterion):\n","    optimizer.zero_grad()\n","\n","    claimIDs, claims, labels, snippets = vals[0], vals[1], torch.tensor(vals[2]).to(DEVICE), vals[3]\n","\n","    logits = model(claims, snippets)\n","    loss = criterion(logits, labels)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss\n","\n","\n","def get_embedding_matrix(generators, dataset, min_occurrence=1):\n","    savename = \"preprocessed/\" + dataset + \"_glove.pkl\"\n","    if os.path.exists(savename):\n","        tmp = pickle.load(open(savename, \"rb\"))\n","        glove_embedding_matrix = tmp[0]\n","        word2idx = tmp[1]\n","        idx2word = tmp[2]\n","        return glove_embedding_matrix, word2idx, idx2word\n","\n","    glove_vectors = GloVe('840B')\n","    all_claims = []\n","    all_snippets = []\n","    for gen in generators:\n","        for vals in gen:\n","            claims = vals[1]\n","            claims = [clean_str(v) for v in claims]\n","            snippets = vals[3]\n","            snippets = [clean_str(item) for sublist in snippets for item in sublist]\n","\n","            all_claims += claims\n","            all_snippets += snippets\n","\n","    all_words = [word for v in all_claims+all_snippets for word in v.split(\" \")]\n","    counter = Counter(all_words)\n","    all_words = set(all_words)\n","    all_words = list(set([word for word in all_words if counter[word] > min_occurrence]))\n","    word2idx = {word: i+2 for i, word in enumerate(all_words)} # reserve 0 for potential mask and 1 for unk token\n","    idx2word = {word2idx[key]: key for key in word2idx}\n","\n","    num_words = len(idx2word)\n","\n","    glove_embedding_matrix = np.random.random((num_words+2, 300)) - 0.5\n","    missed = 0\n","    for word in word2idx:\n","        if word in glove_vectors:\n","            glove_embedding_matrix[word2idx[word]] = glove_vectors[word]\n","        else:\n","            missed += 1\n","\n","    pickle.dump([glove_embedding_matrix, word2idx, idx2word], open(savename, \"wb\"))\n","    return glove_embedding_matrix, word2idx, idx2word\n","\n","def train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename):\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    params = sum([np.prod(p.size()) for p in model_parameters])\n","    print(\"model parameters\", params)\n","\n","    num_epochs = 0\n","    patience_counter = 0\n","    patience_max = 8\n","    best_f1 = -np.inf\n","    while (True):\n","        train_losses = []\n","\n","        model.train()\n","        for ivals, vals in enumerate(train_generator):\n","            loss = train_step(optimizer, vals, model, criterion)\n","            train_losses.append(loss.item())\n","\n","        num_epochs += 1\n","        print_message(\"TRAIN loss\", np.mean(train_losses), num_epochs)\n","\n","        if num_epochs % args.eval_per_epoch == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions = evaluate(val_generator, model)\n","                print_message(\"VALIDATION F1micro, F1macro, loss:\", val_f1micro, val_f1macro, len(val_claimIDs))\n","\n","            if val_f1micro > best_f1:\n","                with torch.no_grad():\n","                    test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions = evaluate(test_generator, model)\n","                    print_message(\"TEST F1micro, F1macro, loss:\", test_f1micro, test_f1macro, len(test_claimIDs))\n","\n","                    other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions = evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")\n","                    print_message(\"OTHER-TEST F1micro, F1macro, loss:\", other_test_f1micro, other_test_f1macro, len(other_test_claimIDs))\n","\n","                    test_remove_top_bottom = []\n","                    test_remove_bottom_top = []\n","                    other_test_remove_top_bottom = []\n","                    other_test_remove_bottom_top = []\n","                    ten = np.arange(10)\n","                    if args.inputtype != \"CLAIM_ONLY\":\n","                        for i in tqdm(range(10)):\n","                            top_is = ten[:(i+1)]\n","                            bottom_is = ten[-(i+1):]\n","                            test_remove_top_bottom.append( evaluate(test_generator, model, ignore_snippet=top_is) )\n","                            test_remove_bottom_top.append( evaluate(test_generator, model, ignore_snippet=bottom_is) )\n","                            other_test_remove_top_bottom.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=top_is))\n","                            other_test_remove_bottom_top.append(evaluate(other_generator, model, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\", ignore_snippet=bottom_is))\n","\n","                        print_message([np.around(v[1], 4) for v in test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in test_remove_bottom_top])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_top_bottom])\n","                        print_message([np.around(v[1], 4) for v in other_test_remove_bottom_top])\n","\n","                patience_counter = 0\n","                best_f1 = val_f1micro\n","                val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","                test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions, test_remove_top_bottom, test_remove_bottom_top]\n","                other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits, other_test_labels, other_test_predictions, other_test_remove_top_bottom, other_test_remove_bottom_top]\n","                misc_store = [args]\n","                total_store = [val_store, test_store, other_test_store, misc_store]\n","            else:\n","                patience_counter += 1\n","\n","            print_message(\"PATIENCE\", patience_counter, \"/\", patience_max)\n","\n","            if patience_counter >= patience_max:\n","                pickle.dump(total_store, open(savename, \"wb\"))\n","                break\n","\n","def run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator):\n","    print(f'***run_bert*** with inputtype {args.inputtype}')\n","    model = MyBertModel.from_pretrained(BERT_MODEL_PATH, labelnum=len(label_order), input_type=inputtype)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","def run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_generator):\n","    print(f'***run_lstm*** with inputtype {args.inputtype}')\n","    glove_embedding_matrix, word2idx, idx2word = get_embedding_matrix([train_generator, val_generator, test_generator, other_generator], args.dataset)\n","\n","    model = LSTMModel(args.lstm_hidden_dim, args.lstm_layers, args.lstm_dropout, len(label_order), word2idx, glove_embedding_matrix, input_type=inputtype)\n","    model.to(DEVICE)\n","\n","    criterion = torch.nn.CrossEntropyLoss(weight=label_weights.to(DEVICE))\n","    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, eps=1e-8)\n","    optimizer.zero_grad()\n","\n","    train_model(model, criterion, optimizer, train_generator, val_generator, test_generator, args, other_generator, savename)\n","\n","def filter_snippet_for_bow(generator, ignore_snippet, inputtype):\n","    samples = []\n","    for vals in generator:\n","        claims = vals[1]\n","        labels = vals[2]\n","        snippets = vals[3]\n","\n","        for i in range(len(snippets)):\n","            snippets[i][ignore_snippet] = \"filler\"\n","\n","        for i in range(len(claims)):\n","            if inputtype == CLAIM_AND_EVIDENCE:\n","                sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","            elif inputtype == CLAIM_ONLY:\n","                sample = clean_str(claims[i])\n","            elif inputtype == EVIDENCE_ONLY:\n","                sample = \" \".join([clean_str(v) for v in snippets[i]])\n","            else:\n","                raise Exception(\"Unknown type\", inputtype)\n","            samples.append(sample)\n","    return samples\n","\n","def get_bows_labels(generators, dataset, inputtype):\n","    all_samples = []\n","    all_labels = []\n","\n","    for gen in generators:\n","        gen_samples = []\n","        gen_labels = []\n","        for vals in gen:\n","            claims = vals[1]\n","            labels = vals[2]\n","            snippets = vals[3]\n","\n","            for i in range(len(claims)):\n","                if inputtype == CLAIM_AND_EVIDENCE:\n","                    sample = clean_str(claims[i]) + \" \".join([clean_str(v) for v in snippets[i]])\n","                elif inputtype == CLAIM_ONLY:\n","                    sample = clean_str(claims[i])\n","                elif inputtype == EVIDENCE_ONLY:\n","                    sample = \" \".join([clean_str(v) for v in snippets[i]])\n","                else:\n","                    raise Exception(\"Unknown type\", inputtype)\n","                gen_samples.append(sample)\n","                gen_labels.append(labels[i])\n","\n","        all_samples.append(gen_samples)\n","        all_labels.append(gen_labels)\n","\n","    test_remove_top_bottom = []\n","    test_remove_bottom_top = []\n","    other_test_remove_top_bottom = []\n","    other_test_remove_bottom_top = []\n","    ten = np.arange(10)\n","    for i in tqdm(range(10)):\n","        top_is = ten[:(i + 1)]\n","        bottom_is = ten[-(i + 1):]\n","        test_remove_top_bottom.append( filter_snippet_for_bow(generators[-2], top_is, inputtype) )\n","        test_remove_bottom_top.append( filter_snippet_for_bow(generators[-2], bottom_is, inputtype) )\n","        other_test_remove_top_bottom.append( filter_snippet_for_bow(generators[-1], top_is, inputtype) )\n","        other_test_remove_bottom_top.append( filter_snippet_for_bow(generators[-1], bottom_is, inputtype) )\n","\n","    vectorizer = TfidfVectorizer(min_df=2)\n","    vectorizer.fit([item for sublist in all_samples for item in sublist])\n","\n","    bows = [vectorizer.transform(all_samples[i]) for i in range(len(all_samples))]\n","\n","    test_remove_top_bottom = [vectorizer.transform(test_remove_top_bottom[i]) for i in range(len(test_remove_top_bottom))]\n","    test_remove_bottom_top = [vectorizer.transform(test_remove_bottom_top[i]) for i in range(len(test_remove_bottom_top))]\n","    other_test_remove_top_bottom = [vectorizer.transform(other_test_remove_top_bottom[i]) for i in range(len(other_test_remove_top_bottom))]\n","    other_test_remove_bottom_top = [vectorizer.transform(other_test_remove_bottom_top[i]) for i in range(len(other_test_remove_bottom_top))]\n","\n","    return bows, all_labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top\n","\n","def run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator):\n","    # print(f'train_generator0 :\\n {next(iter(train_generator))}')\n","    print(f'***run_bow*** with inputtype {args.inputtype}')\n","\n","    bows, labels, test_remove_top_bottom, test_remove_bottom_top, other_test_remove_top_bottom, other_test_remove_bottom_top = get_bows_labels([train_generator, val_generator, test_generator, other_test_generator], args.dataset, inputtype)\n","\n","    train_bow, val_bow, test_bow, other_test_bow = bows[0], bows[1], bows[2], bows[3]\n","    train_labels, val_labels, test_labels, other_test_labels = labels[0], labels[1], labels[2], labels[3]\n","\n","    label_weights = label_weights.numpy()\n","    weights = {i: label_weights[i] for i in range(len(label_weights))}\n","\n","    # print(f'****** run bow train_bow \\n {train_bow}')\n","    # print('*********')\n","\n","    param_grid = [\n","        {'n_estimators': [100, 500, 1000], 'min_samples_leaf': [1, 3, 5, 10], 'min_samples_split': [2, 5, 10]}\n","    ]\n","\n","    opt = GridSearch(model=RandomForestClassifier(n_jobs=5, class_weight=weights), param_grid=param_grid, parallelize=False)\n","\n","    \n","    opt.fit(train_bow, train_labels, val_bow, val_labels, scoring=\"f1_macro\")\n","\n","    def rf_eval(model, bow, labels, other_from=None):\n","        preds = model.predict(bow)\n","\n","        if other_from == \"pomt\": # other data is pomt, and model is trained on snes\n","            # this case is fine\n","            pass\n","        elif other_from == \"snes\": # other data is snes, and model is trained on pomt\n","            # in this case both \"pants on fire!\" and \"false\" should be considered as false\n","            preds[preds == 0] = 1 # 0 is \"pants on fire!\" and 1 is \"false\" for pomt.\n","\n","        f1_macro = f1_score(labels, preds, average=\"macro\")\n","        f1_micro = f1_score(labels, preds, average=\"micro\")\n","        return f1_micro, f1_macro, labels, preds\n","\n","    # val_store = [val_f1micro, val_f1macro, val_claimIDs, val_logits, val_labels, val_predictions]\n","    # test_store = [test_f1micro, test_f1macro, test_claimIDs, test_logits, test_labels, test_predictions,test_remove_top_bottom, test_remove_bottom_top]\n","    # other_test_store = [other_test_f1micro, other_test_f1macro, other_test_claimIDs, other_test_logits,\n","    #                     other_test_labels, other_test_predictions, other_test_remove_top_bottom,\n","    #                     other_test_remove_bottom_top]\n","    #misc_store = [args]\n","\n","\n","    val_store = rf_eval(opt, val_bow, val_labels)\n","    test_store = list(rf_eval(opt, test_bow, test_labels)) + [[rf_eval(opt, test_remove_top_bottom[i], test_labels) for i in range(10)],\n","                                                       [rf_eval(opt, test_remove_bottom_top[i], test_labels) for i in range(10)]]\n","    other_test_store = list(rf_eval(opt, other_test_bow, other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\")) + [[rf_eval(opt, other_test_remove_top_bottom[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)],\n","                                                       [rf_eval(opt, other_test_remove_bottom_top[i], other_test_labels, other_from=\"snes\" if args.dataset == \"pomt\" else \"pomt\") for i in range(10)]]\n","    misc_store = [opt.get_best_params()]\n","    total_store = [val_store, test_store, other_test_store, misc_store]\n","\n","    print_message(\"VALIDATION\", val_store[0], val_store[1])\n","    print_message(\"TEST\", test_store[0], test_store[1])\n","    print_message(\"OTHER-TEST\", other_test_store[0], other_test_store[1])\n","\n","    print_message([np.around(v[1], 4) for v in test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in test_store[-1]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-2]])\n","    print_message([np.around(v[1], 4) for v in other_test_store[-1]])\n","    print(misc_store)\n","\n","    pickle.dump(total_store, open(savename, \"wb\"))\n","\n","def filter_websites(snippets_data):\n","    bad_websites = [\"factcheck.org\", \"politifact.com\", \"snopes.com\", \"fullfact.org\", \"factscan.ca\"]\n","    ids = snippets_data.values[:, 0]\n","    remove_count = 0\n","    for i, id in enumerate(ids):\n","        with open(\"../../multi_fc_publicdata/snippets/\" + id, \"r\", encoding=\"utf-8\") as f:\n","            lines = f.readlines()\n","\n","        links = [line.strip().split(\"\\t\")[-1] for line in lines]\n","        remove = [False for _ in range(10)]\n","        for j in range(len(links)):\n","            remove[j] = any([bad in links[j] for bad in bad_websites])\n","        remove = remove[:10]  # 1 data sample has 11 links by mistake in the dataset\n","        snippets_data.iloc[i, [False] + remove] = \"filler\"\n","\n","        remove_count += np.sum(remove)\n","    print_message(\"REMOVE COUNT\", remove_count)\n","    return snippets_data\n","\n"],"id":"eba4c237","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Error loading SnowballStemmer: Package 'SnowballStemmer'\n","[nltk_data]     not found in index\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5609720e","executionInfo":{"status":"ok","timestamp":1637696461295,"user_tz":-480,"elapsed":11804296,"user":{"displayName":"Wee Yi Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16336291919603696387"}},"outputId":"802ffc85-99fe-432b-cc84-0ba7048c5cfd"},"source":["%%time\n","\n","import gc\n","\n","gc.collect()\n","class vars():\n","    def __init__(self, mode, inputtype):\n","        if mode == \"bow\":\n","            self.dataset = \"snes\"\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bow\"\n","            self.batchsize = 2\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","        elif mode == 'lstm':\n","            self.dataset = \"snes\"\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"lstm\"\n","            self.batchsize = 16\n","            self.eval_per_epoch = 1\n","            self.lr = 0.0001\n","            self.lstm_hidden_dim = 128\n","            self.lstm_layers = 2\n","            self.lstm_dropout = 0.1\n","        elif mode == 'bert':\n","            self.dataset = \"snes\"\n","            self.inputtype = inputtype\n","            self.filter_websites = 0\n","            self.model = \"bert\"\n","            self.batchsize = 6\n","            self.eval_per_epoch = 1\n","            self.lr = 0.000003            \n","\n","filepath = 'sorted.uk.word.unigrams'  \n","word_freq = {}  \n","count = 0\n","with open(filepath, encoding= 'utf-8') as f:\n","    for line in f:\n","        line = line.rstrip()\n","        if line:\n","            x = line.split('\\t')\n","            #print(x)\n","            #print(key, val)\n","            #print(str(x[1]))\n","            word_freq[x[1]] = str(x[0])\n","        count +=1\n","        if count > 100000:\n","            break\n","\n","\n","# for mode in ['bow']:\n","# for mode in ['lstm']:\n","for mode in ['bert']:\n","  for inputtype in ['CLAIM_ONLY', 'CLAIM_AND_EVIDENCE', 'EVIDENCE_ONLY']:\n","  # for inputtype in ['CLAIM_ONLY']:\n","  # for inputtype in ['CLAIM_AND_EVIDENCE']:\n","  # for inputtype in ['EVIDENCE_ONLY']:\n","\n","\n","            \n","    args = vars(mode, inputtype)\n","\n","    if args.filter_websites > 0.5:\n","        savename = \"results/\" + \"-\".join([str(v) for v in [args.filter_websites, args.model, args.dataset, args.inputtype, args.lr, args.batchsize]])\n","    else:\n","        savename = \"results/\" + \"-\".join([str(v) for v in [args.model, args.dataset, args.inputtype, args.lr, args.batchsize]])\n","\n","    if args.model == \"lstm\":\n","        savename += \"-\" + \"-\".join([str(v) for v in [args.lstm_hidden_dim, args.lstm_layers, args.lstm_dropout]])\n","    savename += \".pkl\"\n","\n","    inputtype = INPUT_TYPE_ORDER.index(args.inputtype)\n","    main_data, snippets_data, label_order, splits = load_data(args.dataset)\n","\n","    if args.filter_websites > 0.5:\n","        snippets_data = filter_websites(snippets_data)\n","\n","    params = {\"batch_size\": args.batchsize, \"shuffle\": True, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","    eval_params = {\"batch_size\": args.batchsize, \"shuffle\": False, \"num_workers\": 1, \"collate_fn\": transformer_collate, \"persistent_workers\": True, \"prefetch_factor\":5}\n","\n","    train_generator, val_generator, test_generator, label_weights = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params])\n","\n","    if args.dataset == \"snes\":\n","        main_data, snippets_data, _, splits = load_data(\"pomt\")\n","        if args.filter_websites > 0.5:\n","            snippets_data = filter_websites(snippets_data)\n","        main_data.iloc[main_data.iloc[:, 2] == \"pants on fire!\", 2] = \"false\"\n","        main_data.iloc[main_data.iloc[:, 2] == \"half-true\", 2] = \"mixture\"\n","        _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","    else:\n","        main_data, snippets_data, _, splits = load_data(\"snes\")\n","        if args.filter_websites > 0.5:\n","            snippets_data = filter_websites(snippets_data)\n","        main_data.iloc[main_data.iloc[:, 2] == \"mixture\", 2] = \"half-true\"\n","        _, _, other_test_generator, _ = make_generators(main_data, snippets_data, label_order, splits, [params, eval_params], other_dataset=True)\n","\n","\n","    if args.model == \"bert\":\n","        run_bert(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","    elif args.model == \"lstm\":\n","        run_lstm(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","    elif args.model == \"bow\":\n","        # print(\"run bow\")\n","        run_bow(args, train_generator, val_generator, test_generator, label_weights, inputtype, label_order, savename, other_test_generator)\n","\n","    gc.collect()\n","\n","\n"],"id":"5609720e","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n","The class this function is called from is 'BertTokenizerFast'.\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing MyBertModel: ['distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'predictor.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'predictor.weight', 'encoder.layer.2.attention.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486085\n","[Nov 23, 16:26:44] TRAIN loss 1.6190532757019676 1\n","[Nov 23, 16:26:46] VALIDATION F1micro, F1macro, loss: 0.13214990138067062 0.058697969186689555 507\n","[Nov 23, 16:26:48] TEST F1micro, F1macro, loss: 0.14102564102564102 0.05631117345675309 1014\n","[Nov 23, 16:26:55] OTHER-TEST F1micro, F1macro, loss: 0.15495031284504968 0.07460475232901116 2717\n","[Nov 23, 16:26:55] PATIENCE 0 / 8\n","[Nov 23, 16:27:25] TRAIN loss 1.587994174880756 2\n","[Nov 23, 16:27:26] VALIDATION F1micro, F1macro, loss: 0.42800788954635116 0.16163809753742153 507\n","[Nov 23, 16:27:28] TEST F1micro, F1macro, loss: 0.4812623274161736 0.1830161610923861 1014\n","[Nov 23, 16:27:35] OTHER-TEST F1micro, F1macro, loss: 0.2451232977548767 0.14434773537243656 2717\n","[Nov 23, 16:27:35] PATIENCE 0 / 8\n","[Nov 23, 16:28:05] TRAIN loss 1.5731076732076503 3\n","[Nov 23, 16:28:07] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 23, 16:28:09] TEST F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 1014\n","[Nov 23, 16:28:16] OTHER-TEST F1micro, F1macro, loss: 0.29738682370261316 0.09168794326241134 2717\n","[Nov 23, 16:28:16] PATIENCE 0 / 8\n","[Nov 23, 16:28:47] TRAIN loss 1.5606374487884946 4\n","[Nov 23, 16:28:48] VALIDATION F1micro, F1macro, loss: 0.14595660749506903 0.056996698625657906 507\n","[Nov 23, 16:28:48] PATIENCE 1 / 8\n","[Nov 23, 16:29:19] TRAIN loss 1.5537145397550352 5\n","[Nov 23, 16:29:21] VALIDATION F1micro, F1macro, loss: 0.591715976331361 0.1945082232698744 507\n","[Nov 23, 16:29:21] PATIENCE 2 / 8\n","[Nov 23, 16:29:51] TRAIN loss 1.5391663265590732 6\n","[Nov 23, 16:29:53] VALIDATION F1micro, F1macro, loss: 0.5838264299802761 0.19097555710967787 507\n","[Nov 23, 16:29:53] PATIENCE 3 / 8\n","[Nov 23, 16:30:23] TRAIN loss 1.5106334916844562 7\n","[Nov 23, 16:30:24] VALIDATION F1micro, F1macro, loss: 0.4122287968441814 0.19656757477925674 507\n","[Nov 23, 16:30:24] PATIENCE 4 / 8\n","[Nov 23, 16:30:54] TRAIN loss 1.4421320213760074 8\n","[Nov 23, 16:30:56] VALIDATION F1micro, F1macro, loss: 0.3076923076923077 0.1664872224549644 507\n","[Nov 23, 16:30:56] PATIENCE 5 / 8\n","[Nov 23, 16:31:25] TRAIN loss 1.3450691739751681 9\n","[Nov 23, 16:31:27] VALIDATION F1micro, F1macro, loss: 0.358974358974359 0.19636299931732296 507\n","[Nov 23, 16:31:27] PATIENCE 6 / 8\n","[Nov 23, 16:31:57] TRAIN loss 1.2003854079977483 10\n","[Nov 23, 16:31:58] VALIDATION F1micro, F1macro, loss: 0.42209072978303747 0.216480216802168 507\n","[Nov 23, 16:31:58] PATIENCE 7 / 8\n","[Nov 23, 16:32:28] TRAIN loss 1.040448720466245 11\n","[Nov 23, 16:32:29] VALIDATION F1micro, F1macro, loss: 0.27613412228796846 0.17321178204546317 507\n","[Nov 23, 16:32:29] PATIENCE 8 / 8\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype CLAIM_AND_EVIDENCE\n"]},{"output_type":"stream","name":"stderr","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n","The class this function is called from is 'BertTokenizerFast'.\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing MyBertModel: ['distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'attn_score.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'predictor.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'attn_score.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'predictor.weight', 'encoder.layer.2.attention.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109490694\n","[Nov 23, 16:37:41] TRAIN loss 1.65799579799578 1\n","[Nov 23, 16:37:51] VALIDATION F1micro, F1macro, loss: 0.38461538461538464 0.15751025991792064 507\n","[Nov 23, 16:38:11] TEST F1micro, F1macro, loss: 0.4122287968441814 0.16982693902578028 1014\n","[Nov 23, 16:39:05] OTHER-TEST F1micro, F1macro, loss: 0.23739418476260585 0.13068671416809025 2717\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [22:01<00:00, 132.18s/it]"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 17:01:06] [0.1558, 0.1421, 0.1329, 0.1254, 0.118, 0.1066, 0.0939, 0.086, 0.0816, 0.0816]\n","[Nov 23, 17:01:06] [0.1692, 0.157, 0.1425, 0.136, 0.1267, 0.1158, 0.1052, 0.0949, 0.086, 0.0816]\n","[Nov 23, 17:01:06] [0.1276, 0.1226, 0.1184, 0.1144, 0.1101, 0.1042, 0.0972, 0.0888, 0.0846, 0.0847]\n","[Nov 23, 17:01:06] [0.1301, 0.1259, 0.1212, 0.1174, 0.1116, 0.1083, 0.1016, 0.0952, 0.087, 0.0847]\n","[Nov 23, 17:01:06] PATIENCE 0 / 8\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 17:04:23] TRAIN loss 1.6091922242697831 2\n","[Nov 23, 17:04:32] VALIDATION F1micro, F1macro, loss: 0.1282051282051282 0.09107094114338551 507\n","[Nov 23, 17:04:32] PATIENCE 1 / 8\n","[Nov 23, 17:07:49] TRAIN loss 1.6034194109206263 3\n","[Nov 23, 17:07:59] VALIDATION F1micro, F1macro, loss: 0.13609467455621302 0.05496448687205642 507\n","[Nov 23, 17:07:59] PATIENCE 2 / 8\n","[Nov 23, 17:11:16] TRAIN loss 1.5914153811093923 4\n","[Nov 23, 17:11:26] VALIDATION F1micro, F1macro, loss: 0.23274161735700197 0.1057033082181528 507\n","[Nov 23, 17:11:26] PATIENCE 3 / 8\n","[Nov 23, 17:14:43] TRAIN loss 1.558961503107 5\n","[Nov 23, 17:14:52] VALIDATION F1micro, F1macro, loss: 0.17357001972386588 0.09083712348751394 507\n","[Nov 23, 17:14:52] PATIENCE 4 / 8\n","[Nov 23, 17:18:09] TRAIN loss 1.5623787240804852 6\n","[Nov 23, 17:18:18] VALIDATION F1micro, F1macro, loss: 0.3747534516765286 0.1766302573769662 507\n","[Nov 23, 17:18:18] PATIENCE 5 / 8\n","[Nov 23, 17:21:35] TRAIN loss 1.5251937736631245 7\n","[Nov 23, 17:21:44] VALIDATION F1micro, F1macro, loss: 0.5167652859960552 0.20962038404726738 507\n","[Nov 23, 17:22:04] TEST F1micro, F1macro, loss: 0.5433925049309665 0.2321304007921488 1014\n","[Nov 23, 17:22:57] OTHER-TEST F1micro, F1macro, loss: 0.2517482517482518 0.15972871871943922 2717\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [21:54<00:00, 131.46s/it]"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 17:44:52] [0.2279, 0.2366, 0.2339, 0.2283, 0.2239, 0.221, 0.2191, 0.2196, 0.2184, 0.218]\n","[Nov 23, 17:44:52] [0.2314, 0.2293, 0.2342, 0.2357, 0.2299, 0.2234, 0.2206, 0.2211, 0.2201, 0.218]\n","[Nov 23, 17:44:52] [0.1633, 0.1635, 0.1637, 0.1642, 0.1654, 0.1667, 0.1671, 0.1671, 0.169, 0.1687]\n","[Nov 23, 17:44:52] [0.1607, 0.1633, 0.1644, 0.1642, 0.1645, 0.1662, 0.166, 0.1675, 0.1675, 0.1687]\n","[Nov 23, 17:44:52] PATIENCE 0 / 8\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 17:48:07] TRAIN loss 1.48570760868087 8\n","[Nov 23, 17:48:17] VALIDATION F1micro, F1macro, loss: 0.46942800788954636 0.23504943136369408 507\n","[Nov 23, 17:48:17] PATIENCE 1 / 8\n","[Nov 23, 17:51:32] TRAIN loss 1.408308107154192 9\n","[Nov 23, 17:51:42] VALIDATION F1micro, F1macro, loss: 0.40236686390532544 0.1984895653104894 507\n","[Nov 23, 17:51:42] PATIENCE 2 / 8\n","[Nov 23, 17:54:57] TRAIN loss 1.3046370643518261 10\n","[Nov 23, 17:55:07] VALIDATION F1micro, F1macro, loss: 0.3609467455621302 0.2160158690281075 507\n","[Nov 23, 17:55:07] PATIENCE 3 / 8\n","[Nov 23, 17:58:22] TRAIN loss 1.1290360087075748 11\n","[Nov 23, 17:58:32] VALIDATION F1micro, F1macro, loss: 0.5680473372781065 0.21272550886343328 507\n","[Nov 23, 17:58:51] TEST F1micro, F1macro, loss: 0.5927021696252466 0.23269166028164148 1014\n","[Nov 23, 17:59:44] OTHER-TEST F1micro, F1macro, loss: 0.2701509017298491 0.15836287033826663 2717\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [21:44<00:00, 130.43s/it]"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 18:21:29] [0.2333, 0.2316, 0.2301, 0.2301, 0.2324, 0.2345, 0.2339, 0.2358, 0.239, 0.239]\n","[Nov 23, 18:21:29] [0.2327, 0.2336, 0.2307, 0.2304, 0.2308, 0.23, 0.2374, 0.2366, 0.2397, 0.239]\n","[Nov 23, 18:21:29] [0.1597, 0.1595, 0.1603, 0.1608, 0.1623, 0.1639, 0.1647, 0.1656, 0.1661, 0.1664]\n","[Nov 23, 18:21:29] [0.1583, 0.1597, 0.1615, 0.1599, 0.1624, 0.1652, 0.1662, 0.1662, 0.1664, 0.1664]\n","[Nov 23, 18:21:29] PATIENCE 0 / 8\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 18:24:43] TRAIN loss 0.9682518116928436 12\n","[Nov 23, 18:24:52] VALIDATION F1micro, F1macro, loss: 0.38461538461538464 0.2209251894758076 507\n","[Nov 23, 18:24:52] PATIENCE 1 / 8\n","[Nov 23, 18:28:07] TRAIN loss 0.8086056464376885 13\n","[Nov 23, 18:28:16] VALIDATION F1micro, F1macro, loss: 0.25443786982248523 0.1625078827950278 507\n","[Nov 23, 18:28:16] PATIENCE 2 / 8\n","[Nov 23, 18:31:31] TRAIN loss 0.7212922797865562 14\n","[Nov 23, 18:31:40] VALIDATION F1micro, F1macro, loss: 0.3333333333333333 0.19834652487379228 507\n","[Nov 23, 18:31:40] PATIENCE 3 / 8\n","[Nov 23, 18:34:54] TRAIN loss 0.5516541581443587 15\n","[Nov 23, 18:35:04] VALIDATION F1micro, F1macro, loss: 0.41420118343195267 0.22523719931498604 507\n","[Nov 23, 18:35:04] PATIENCE 4 / 8\n","[Nov 23, 18:38:18] TRAIN loss 0.461321419019982 16\n","[Nov 23, 18:38:28] VALIDATION F1micro, F1macro, loss: 0.33727810650887574 0.19414478991116968 507\n","[Nov 23, 18:38:28] PATIENCE 5 / 8\n","[Nov 23, 18:41:42] TRAIN loss 0.39425390717538855 17\n","[Nov 23, 18:41:52] VALIDATION F1micro, F1macro, loss: 0.23865877712031558 0.1663070979692564 507\n","[Nov 23, 18:41:52] PATIENCE 6 / 8\n","[Nov 23, 18:45:06] TRAIN loss 0.3332272865663981 18\n","[Nov 23, 18:45:16] VALIDATION F1micro, F1macro, loss: 0.5660749506903353 0.21315936168839905 507\n","[Nov 23, 18:45:16] PATIENCE 7 / 8\n","[Nov 23, 18:48:30] TRAIN loss 0.31351599568635624 19\n","[Nov 23, 18:48:40] VALIDATION F1micro, F1macro, loss: 0.4161735700197239 0.22509863948569936 507\n","[Nov 23, 18:48:40] PATIENCE 8 / 8\n","len 5069 , false (64.3\\%), mostly false (7.5\\%), mixture (12.3\\%), mostly true (2.8\\%), true (13.0\\%)\n","len 13581 , false (29.7\\%), mostly false (17.0\\%), mixture (19.8\\%), mostly true (18.8\\%), true (14.8\\%)\n","***run_bert*** with inputtype EVIDENCE_ONLY\n"]},{"output_type":"stream","name":"stderr","text":["You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n","The class this function is called from is 'BertTokenizerFast'.\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing MyBertModel: ['distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias']\n","- This IS expected if you are initializing MyBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MyBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of MyBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['encoder.layer.11.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'pooler.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'attn_score.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'predictor.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.bias', 'attn_score.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'pooler.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.bias', 'predictor.weight', 'encoder.layer.2.attention.output.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["model parameters 109486854\n","[Nov 23, 18:52:40] TRAIN loss 1.6358572519006762 1\n","[Nov 23, 18:52:46] VALIDATION F1micro, F1macro, loss: 0.1242603550295858 0.04421052631578947 507\n","[Nov 23, 18:52:59] TEST F1micro, F1macro, loss: 0.1232741617357002 0.043898156277436345 1014\n","[Nov 23, 18:53:32] OTHER-TEST F1micro, F1macro, loss: 0.19764446080235554 0.06601106330669945 2717\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [13:08<00:00, 78.81s/it]"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 19:06:40] [0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439]\n","[Nov 23, 19:06:40] [0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439, 0.0439]\n","[Nov 23, 19:06:40] [0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066]\n","[Nov 23, 19:06:40] [0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066, 0.066]\n","[Nov 23, 19:06:40] PATIENCE 0 / 8\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 19:08:50] TRAIN loss 1.6023496229302239 2\n","[Nov 23, 19:08:56] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 23, 19:09:09] TEST F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 1014\n","[Nov 23, 19:09:42] OTHER-TEST F1micro, F1macro, loss: 0.29738682370261316 0.09168794326241134 2717\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [13:07<00:00, 78.71s/it]"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 19:22:49] [0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565]\n","[Nov 23, 19:22:49] [0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565, 0.1565]\n","[Nov 23, 19:22:49] [0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917]\n","[Nov 23, 19:22:49] [0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917]\n","[Nov 23, 19:22:49] PATIENCE 0 / 8\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["[Nov 23, 19:24:59] TRAIN loss 1.5900437697969578 3\n","[Nov 23, 19:25:05] VALIDATION F1micro, F1macro, loss: 0.20512820512820512 0.09234653460286309 507\n","[Nov 23, 19:25:05] PATIENCE 1 / 8\n","[Nov 23, 19:27:15] TRAIN loss 1.5863927307765227 4\n","[Nov 23, 19:27:22] VALIDATION F1micro, F1macro, loss: 0.631163708086785 0.17479475040450648 507\n","[Nov 23, 19:27:22] PATIENCE 2 / 8\n","[Nov 23, 19:29:32] TRAIN loss 1.59145012185783 5\n","[Nov 23, 19:29:38] VALIDATION F1micro, F1macro, loss: 0.1242603550295858 0.04421052631578947 507\n","[Nov 23, 19:29:38] PATIENCE 3 / 8\n","[Nov 23, 19:31:48] TRAIN loss 1.5771648790183905 6\n","[Nov 23, 19:31:54] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 23, 19:31:54] PATIENCE 4 / 8\n","[Nov 23, 19:34:04] TRAIN loss 1.575556547959914 7\n","[Nov 23, 19:34:11] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 23, 19:34:11] PATIENCE 5 / 8\n","[Nov 23, 19:36:21] TRAIN loss 1.58578383268134 8\n","[Nov 23, 19:36:27] VALIDATION F1micro, F1macro, loss: 0.6429980276134122 0.1565426170468187 507\n","[Nov 23, 19:36:27] PATIENCE 6 / 8\n","[Nov 23, 19:38:37] TRAIN loss 1.5716967453827728 9\n","[Nov 23, 19:38:43] VALIDATION F1micro, F1macro, loss: 0.15384615384615385 0.06306233162342695 507\n","[Nov 23, 19:38:43] PATIENCE 7 / 8\n","[Nov 23, 19:40:53] TRAIN loss 1.5707611414427693 10\n","[Nov 23, 19:41:00] VALIDATION F1micro, F1macro, loss: 0.6193293885601578 0.1842692307692308 507\n","[Nov 23, 19:41:00] PATIENCE 8 / 8\n","CPU times: user 3h 33min 53s, sys: 4min 27s, total: 3h 38min 21s\n","Wall time: 3h 16min 44s\n"]}]}]}